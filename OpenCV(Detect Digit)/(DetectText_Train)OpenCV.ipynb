{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split # for splitting data\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator # for image augmentation\n",
    "from keras.utils.np_utils import to_categorical # for one-hot encoding\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, \\\n",
    "                                    MaxPooling2D, \\\n",
    "                                    Flatten, \\\n",
    "                                    Dense, \\\n",
    "                                    Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mnist_png/training'\n",
    "images = list()\n",
    "class_number_labels = list()\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2 \n",
    "num_of_imgs_each_list = list()\n",
    "image_dimensions = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "list_of_classes = os.listdir(path)\n",
    "num_of_classes = len(list_of_classes)\n",
    "print(\"Number of classes: \", num_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "# getting access to each class\n",
    "for class_num in range (0, num_of_classes):\n",
    "    # list of images in each class\n",
    "    list_of_images = os.listdir(path + \"/\" + str(class_num))\n",
    "    # getting access to each image in each class\n",
    "    for img_name in list_of_images:\n",
    "        \n",
    "        img = cv2.imread(path + \"/\" + str(class_num) + \"/\" + img_name)\n",
    "        img = cv2.resize(img, (image_dimensions[0], image_dimensions[1]))\n",
    "        images.append(img)\n",
    "        class_number_labels.append(class_num)\n",
    "\n",
    "print(len(images))\n",
    "print(len(class_number_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 3)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "images = np.array(images) # converting to numpy array\n",
    "class_number_labels = np.array(class_number_labels) # converting to numpy array\n",
    "print(images.shape)\n",
    "print(class_number_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38400, 32, 32, 3)\n",
      "(12000, 32, 32, 3)\n",
      "(9600, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# splitting data into train, test and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(images, class_number_labels, test_size=test_ratio)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_ratio)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3748, 4304, 3824, 3891, 3739, 3508, 3819, 4027, 3763, 3777]\n"
     ]
    }
   ],
   "source": [
    "# creating a list that holds the number of images in each class\n",
    "for num_of_imgs_each_name in range(0, num_of_classes):\n",
    "    num_of_imgs_each = len(np.where(Y_train==num_of_imgs_each_name)[0]) # getting the length of the list of appearances of each number (by index)\n",
    "    num_of_imgs_each_list.append(num_of_imgs_each)\n",
    "\n",
    "print(num_of_imgs_each_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the number of images in each class\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(0, num_of_classes), num_of_imgs_each_list)\n",
    "plt.title(\"Number of images per class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.equalizeHist(img) # distribute the lightness of the image evenly\n",
    "    img = img/255 # better for training processes\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the pre-processing function to each image of the training, testing, and valdiation sets\n",
    "X_train = np.array(list(map(pre_processing, X_train)))\n",
    "X_test = np.array(list(map(pre_processing, X_test)))\n",
    "X_validation = np.array(list(map(pre_processing, X_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the depth that is needed for the CNN to run properly to each image in each set\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment images (add zoom, rotation, etc.)\n",
    "# This makes the dataset more generic\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    zoom_range=0.2,\n",
    "                                    shear_range=0.1,\n",
    "                                    rotation_range=10)\n",
    "\n",
    "# we don't generate images before training but generate as we go along during the train process\n",
    "# we want the generator to know the dataset beforehand\n",
    "# the images would be in batches\n",
    "# we request a batch, it would augment, and send it back\n",
    "data_generator.fit(X_train)\n",
    "\n",
    "# one hot encoding our matrices is necessary for the network\n",
    "Y_train = to_categorical(Y_train, num_of_classes)\n",
    "Y_test = to_categorical(Y_test, num_of_classes)\n",
    "Y_validation = to_categorical(Y_validation, num_of_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an optimizer\n",
    "my_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# the model is based on the LeNet model\n",
    "def my_model():\n",
    "    num_of_filters = 60\n",
    "    size_of_filter_1 = (5, 5)\n",
    "    size_of_filter_2 = (3, 3)\n",
    "    size_of_pool = (2, 2)\n",
    "    num_of_nodes = 500\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # adding the 1st convolutional layer\n",
    "    model.add((Conv2D(num_of_filters, size_of_filter_1, input_shape=(image_dimensions[0], image_dimensions[1], 1), activation='relu')))\n",
    "    # adding the 2nd convolutional layer\n",
    "    model.add((Conv2D(num_of_filters, size_of_filter_1, activation='relu')))\n",
    "    # adding a pooling layer \n",
    "    model.add(MaxPooling2D(pool_size=size_of_pool))\n",
    "    # adding other convolutional layers\n",
    "    model.add((Conv2D(num_of_filters//2, size_of_filter_2, activation='relu')))\n",
    "    model.add((Conv2D(num_of_filters//2, size_of_filter_2, activation='relu')))\n",
    "    # adding another pooling layer \n",
    "    model.add(MaxPooling2D(pool_size=size_of_pool))\n",
    "    # adding a dropout layer(helps to reduce overfitting and it's making it more generic)\n",
    "    model.add(Dropout(0.5))\n",
    "    # adding a flattening layer\n",
    "    model.add(Flatten())\n",
    "    # adding a dense layer\n",
    "    model.add(Dense(num_of_nodes, activation='relu'))\n",
    "    # adding another dropout layer\n",
    "    model.add(Dropout(0.5))\n",
    "    # adding another dense layer\n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "    # compiling the model and mentioning the optimizer and loss function (cross entropy) and metrics (accuracy)\n",
    "    model.compile(my_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 28, 28, 60)        1560      \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 24, 24, 60)        90060     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 12, 12, 60)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 10, 10, 30)        16230     \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 8, 8, 30)          8130      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 4, 4, 30)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 4, 4, 30)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 500)               240500    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361,490\n",
      "Trainable params: 361,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# creating our model\n",
    "model = my_model()\n",
    "# checking the model summary to see the parameters of the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables used in training and requesting\n",
    "batch_size_val = 50\n",
    "epochs_val = 10\n",
    "steps_per_epoch_val = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the training at the same time requesting in batches that we need images\n",
    "history = model.fit_generator(data_generator.flow(\n",
    "                                                  X_train, \n",
    "                                                  Y_train, \n",
    "                                                  batch_size=batch_size_val\n",
    "                                                 ), \n",
    "                              steps_per_epoch=steps_per_epoch_val, \n",
    "                              epochs=epochs_val, \n",
    "                              validation_data=(\n",
    "                                               X_validation, \n",
    "                                               Y_validation\n",
    "                                              ),\n",
    "                              shuffle=1            \n",
    "                             )\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting grphs to see the variation in accuracy and loss\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# checking the score of the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print(\"Test score: \", score[0]) # printing the test score\n",
    "print(\"Test accuracy: \", score[1]) # printing the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file so that we can use it in our testing model \n",
    "# saving the model as a pickle object\n",
    "pickle_out = open(\"trained_model.p\", \"wb\")\n",
    "pickle.dump(model, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned from the YouTube channel: Murtaza's Workshop<br><br>\n",
    "Text Detection using Neural Networks | OPENCV Python"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
